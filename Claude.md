# CRM B2Dev - Scraping Google Maps

## ğŸš¨ RÃˆGLES CRITIQUES
- TOUJOURS vÃ©rifier les doublons avant insertion en BDD (tÃ©lÃ©phone, email, siteWeb)
- GÃ©rer les doublons **pendant le scraping** ET **pendant l'import Excel**
- JAMAIS scraper plus de 100 prospects par ville
- Ne scraper QUE les entreprises sans site valide ou avec site obsolÃ¨te
- Les tÃ©lÃ©phones doivent Ãªtre formatÃ©s sans espaces : "0612345678"
- Tous les commentaires et messages en FRANÃ‡AIS

## ğŸ“‹ Ã€ PROPOS DU PROJET
CRM pour identifier et contacter des artisans/PME avec des sites web obsolÃ¨tes ou inexistants.
L'objectif est de leur proposer les services de crÃ©ation de sites web de B2Dev.

Oui exactement ! Je vais t'aider Ã  les mettre Ã  jour.
ğŸ“ Mise Ã  jour du CLAUDE.md
Ajoute cette section juste aprÃ¨s "Ã€ PROPOS DU PROJET" :
markdown## ğŸ” AUTHENTIFICATION

**SystÃ¨me : Next-Auth avec emails autorisÃ©s**

**Fonctionnement :**
- Liste d'emails autorisÃ©s dans le code (AUTHORIZED_EMAILS)
- Connexion via Google OAuth uniquement
- Si email pas dans la liste â†’ AccÃ¨s refusÃ©
- Si email autorisÃ© â†’ AccÃ¨s complet au CRM

**Emails autorisÃ©s actuels :**
- amaury.allemand@example.com (Amaury - Commercial)
- partenaire.technique@example.com (Partenaire - Dev)
- (Ã  complÃ©ter avec vrais emails)

**Pages publiques :**
- `/login` : Page de connexion Google
- `/api/auth/*` : Routes Next-Auth

**Pages protÃ©gÃ©es (toutes les autres) :**
- `/scraping` : Scraping Google Maps
- `/leads` : Gestion des leads
- `/api/*` : Toutes les API routes

**Configuration :**
- Provider : Google OAuth
- Variables d'env nÃ©cessaires :
  - `GOOGLE_CLIENT_ID`
  - `GOOGLE_CLIENT_SECRET`
  - `NEXTAUTH_SECRET`
  - `NEXTAUTH_URL`

**Ajout/Retrait de membres :**
- Modifier le tableau `AUTHORIZED_EMAILS` dans `lib/auth-config.ts`
- RedÃ©ployer l'application

**Utilisateurs :**
- Amaury : gestion commerciale et relation client
- Partenaire technique : dÃ©veloppement des sites

**Stack technique :**
- Next.js 14 (App Router)
- TypeScript (strict mode)
- PostgreSQL via Supabase
- Prisma ORM
- Playwright pour scraping Google Maps
- Tailwind CSS
- xlsx pour import/export Excel

## ğŸ—ï¸ STRUCTURE DU PROJETapp/
â”œâ”€â”€ page.tsx                    # Redirection vers /scraping
â”œâ”€â”€ scraping/
â”‚   â””â”€â”€ page.tsx                # PAGE SCRAPING avec formulaire
â”œâ”€â”€ leads/
â”‚   â”œâ”€â”€ page.tsx                # PAGE LEADS avec tableau + import Excel
â”‚   â””â”€â”€ [id]/page.tsx           # DÃ©tail d'un lead
â””â”€â”€ api/
â”œâ”€â”€ scrape/route.ts         # POST: Lancer le scraping
â”œâ”€â”€ leads/
â”‚   â”œâ”€â”€ route.ts            # GET: liste leads, POST: crÃ©er lead
â”‚   â”œâ”€â”€ [id]/route.ts       # GET/PUT/DELETE: lead individuel
â”‚   â””â”€â”€ import/route.ts     # POST: Import fichier Excel
â””â”€â”€ export/route.ts         # GET: Export Excellib/
â”œâ”€â”€ scraper.ts                  # Logique Playwright pour Google Maps
â”œâ”€â”€ site-validator.ts           # Validation qualitÃ© site web
â”œâ”€â”€ database.ts                 # Fonctions Prisma rÃ©utilisables
â”œâ”€â”€ excel-parser.ts             # Parse fichiers Excel uploadÃ©s
â”œâ”€â”€ validators.ts               # Validation et nettoyage des donnÃ©es
â””â”€â”€ types.ts                    # Types TypeScript partagÃ©sprisma/
â”œâ”€â”€ schema.prisma               # SchÃ©ma de la base de donnÃ©es
â””â”€â”€ migrations/                 # Historique des migrationscomponents/
â”œâ”€â”€ ScrapingForm.tsx            # Formulaire scraping (mÃ©tier + villes)
â”œâ”€â”€ LeadsTable.tsx              # Tableau des leads avec filtres
â”œâ”€â”€ LeadRow.tsx                 # Ligne de tableau lead
â”œâ”€â”€ ImportExcel.tsx             # Upload + import Excel
â””â”€â”€ StatusSelect.tsx            # Select pour statut lead

## ğŸ¯ PAGE SCRAPING

### Configuration Anti-Ban Optimale

**DÃ©lais (millisecondes) :**
- `PAGE_LOAD`: 3000ms - AprÃ¨s chargement Google Maps
- `AFTER_SEARCH`: 3000ms - AprÃ¨s lancement recherche
- `BETWEEN_SCROLLS`: 2000ms - Entre chaque scroll
- `BETWEEN_CITIES`: 2500ms - Entre chaque ville (randomisÃ© 2000-3000ms)
- `TIMEOUT`: 30000ms - Timeout max par page

**Rate Limiting :**
- 2-3 secondes entre chaque ville (RANDOMISÃ‰ pour Ã©viter pattern bot)
- 2 secondes entre chaque scroll
- Randomisation : +/- 500ms sur tous les dÃ©lais

**Techniques Anti-DÃ©tection :**
- âœ… Playwright Stealth mode activÃ© (masque les signaux bot)
- âœ… User-Agent rÃ©aliste : Chrome/Mac OS X
- âœ… Mode headless (plus rapide, moins dÃ©tectable)
- âœ… DÃ©lais randomisÃ©s (Ã©viter patterns fixes)
- âœ… Pas de connexion compte Google
- âœ… Viewport rÃ©aliste : 1920x1080

**Limites Quotidiennes RecommandÃ©es :**
- Maximum 20-30 villes par jour (safe)
- Maximum 100 prospects par ville
- Ã‰viter de scraper la mÃªme ville plusieurs fois par jour

### Logique du Scraping

**FonctionnalitÃ©s :**
- Zone de texte : "Type d'entreprise" (ex: "plombier", "Ã©lectricien")
- Zone de texte : "Villes" (ex: "Paris, Lyon, Marseille")
- Bouton "Lancer le scraping"
- Affichage temps rÃ©el des prospects trouvÃ©s
- Afficher une barre de chargement
- Compteur par ville (max 100)

**CritÃ¨res de sÃ©lection (Motif SÃ©lection) :**
1. âŒ **Pas de site web** â†’ "Pas de site"
2. âŒ **Site sur annuaire** (PagesJaunes, Yelp, etc.) â†’ "Site annuaire"
3. âŒ **Site rÃ©seaux sociaux uniquement** (Facebook, Instagram) â†’ "RÃ©seaux sociaux uniquement"
4. âŒ **Site plateforme** (Travaux.com, HomeAdvisor, etc.) â†’ "Site plateforme"
5. âŒ **Site non mobile-friendly** â†’ "Non responsive"
6. âŒ **Site obsolÃ¨te** (avant 2018) â†’ "Site obsolÃ¨te"

**Sites Ã  rejeter (ne PAS scraper) :**
- Sites normaux et rÃ©cents (aprÃ¨s 2018)
- Sites responsive et bien conÃ§us
- Sites e-commerce professionnels

## ğŸ¯ PAGE LEADS

**Colonnes du tableau :**
| Colonne | Type | Description |
|---------|------|-------------|
| Nom | Texte | Nom de l'entreprise |
| TÃ©lÃ©phone | Texte | Format: 0612345678 |
| Site Web | Texte | URL complÃ¨te ou "Aucun" |
| Adresse | Texte | Adresse complÃ¨te |
| Ville | Texte | Ville (utilisÃ©e pour filtre) |
| Motif SÃ©lection | Texte | Pourquoi on l'a sÃ©lectionnÃ© |
| Statut | Select | Ã‰tat du lead (voir ci-dessous) |
| Note | Texte long | Notes libres |

**Statuts possibles (dans cet ordre) :**
1. ğŸ” A contacter
2. ğŸ“… RDV maquette
3. ğŸ“„ Envoie Devis
4. â³ Attente d'acompte
5. ğŸ’° Acompte payÃ©
6. ğŸ¨ RDV de mis projet
7. ğŸ“ RDV de fin de projet + Formation
8. âœ… Finis
9. ğŸ›‘ Perdu

**FonctionnalitÃ©s :**
- Tableau avec tri et filtres (par ville, statut, motif)
- Bouton "Importer Excel" â†’ Upload fichier .xlsx
- Bouton "Exporter Excel" â†’ TÃ©lÃ©charge tous les leads
- Clic sur ligne â†’ Ouvre dÃ©tail du lead
- Modification inline du statut
- DÃ©tection automatique des doublons lors de l'import

## ğŸ—ƒï¸ BASE DE DONNÃ‰ES

**Table : leads**
```prismamodel Lead {
id              String   @id @default(uuid())
nom             String
telephone       String?  @unique
email           String?  @unique
siteWeb         String?  @unique
adresse         String?
ville           String
codePostal      String?
metier          String   // Ex: "plombier"
motifSelection  String   // Ex: "Pas de site", "Site obsolÃ¨te"
statut          String   @default("A contacter")
note            String?  @db.Text
noteGoogle      Float?   // Note Google (1-5)
nombreAvis      Int?     // Nombre d'avis Google
createdAt       DateTime @default(now())
updatedAt       DateTime @updatedAt@@index([ville])
@@index([statut])
@@index([metier])
}

## ğŸ” LOGIQUE DE SCRAPING

**Ã‰tapes du scraping :**
1. Parse les villes (split par virgule)
2. Pour chaque ville :
   - Recherche Google Maps : "{mÃ©tier} {ville}"
   - Scroll pour charger rÃ©sultats
   - **Limite : 100 prospects max par ville**
   - Extraire : nom, tÃ©lÃ©phone, site, adresse, note, avis
3. **Validation du site web :**
   - Si pas de site â†’ âœ… Ajouter (motif: "Pas de site")
   - Si site annuaire (pagesjaunes.fr, yelp.fr) â†’ âœ… Ajouter (motif: "Site annuaire")
   - Si rÃ©seaux sociaux uniquement (facebook.com, instagram.com) â†’ âœ… Ajouter (motif: "RÃ©seaux sociaux uniquement")
   - Si site plateforme (travaux.com, homeadvisor.fr) â†’ âœ… Ajouter (motif: "Site plateforme")
   - Si site existe et valide â†’ VÃ©rifier responsive + date
     - Non responsive â†’ âœ… Ajouter (motif: "Non responsive")
     - Date < 2018 â†’ âœ… Ajouter (motif: "Site obsolÃ¨te")
     - Sinon â†’ âŒ Rejeter (bon site)
4. **VÃ©rification doublons en temps rÃ©el** (avant ajout en BDD)
   - Check tÃ©lÃ©phone, email, siteWeb dans la BDD
   - Si existe dÃ©jÃ  â†’ Skip
5. Insertion en BDD

**Playwright config :**
- Mode headless par dÃ©faut
- Timeout : 30 secondes/page
- Rate limiting : 2 secondes entre requÃªtes
- User-agent : navigateur classique

## ğŸ“¥ IMPORT EXCEL

**Format attendu (fichier comme prospects_plombier_*.xlsx) :**
| Nom | TÃ©lÃ©phone | Site Web | Adresse | Ville | Motif SÃ©lection | Statut | Note |
|-----|-----------|----------|---------|-------|-----------------|--------|------|
| ... | ... | ... | ... | ... | ... | ... | ... |

**Logique d'import :**
1. Upload fichier .xlsx
2. Parse avec bibliothÃ¨que `xlsx`
3. Pour chaque ligne :
   - Nettoyer les donnÃ©es (tÃ©lÃ©phone sans espaces, etc.)
   - **VÃ©rifier doublons** (tÃ©lÃ©phone, email, siteWeb)
   - Si doublon â†’ Skip avec log
   - Si nouveau â†’ InsÃ©rer en BDD
4. Retourner rÃ©sumÃ© : "X leads importÃ©s, Y doublons ignorÃ©s"

## ğŸ“¤ EXPORT EXCEL

**FonctionnalitÃ© :**
- Bouton "Exporter vers Excel"
- GÃ©nÃ¨re fichier : `leads_export_{date}.xlsx`
- Contient toutes les colonnes du tableau
- Format identique aux imports (rÃ©utilisable)

## ğŸ¯ CONVENTIONS DE CODE
- TypeScript strict mode activÃ©
- Async/await pour toutes les opÃ©rations asynchrones
- Nommage : 
  - camelCase pour variables et fonctions
  - PascalCase pour composants React et types
  - kebab-case pour fichiers
- Toujours utiliser try/catch pour les opÃ©rations Playwright et Prisma
- Gestion d'erreur : retourner des objets `{ success: boolean, data?: T, error?: string }`
- Commentaires en franÃ§ais dans le code

## âš¡ COMMANDES IMPORTANTES
```bashnpm run dev              # Serveur dev (http://localhost:3000)
npm run build            # Build production
npx prisma migrate dev   # CrÃ©er une migration
npx prisma studio        # Interface visuelle BDD
npx prisma generate      # RÃ©gÃ©nÃ©rer le client Prisma

## âœ… WORKFLOW DE DÃ‰VELOPPEMENT
1. Claude propose un plan d'action dÃ©taillÃ©
2. Amaury valide ou demande des ajustements
3. Claude implÃ©mente Ã©tape par Ã©tape avec explications
4. Tests manuels par Amaury
5. Commit avec message descriptif en franÃ§ais

## ğŸ“Š QUALITÃ‰ DU CODE
- Toujours vÃ©rifier les types TypeScript (pas de `any`)
- Tester les edge cases (pas de tÃ©lÃ©phone, pas de site, erreurs rÃ©seau)
- GÃ©rer les erreurs proprement avec messages clairs en franÃ§ais
- Logs dÃ©taillÃ©s pour debugging
- Interface utilisateur intuitive et responsive

## ğŸš€ DÃ‰PLOIEMENT (plus tard)
- Vercel pour le frontend + API Routes
- Supabase pour PostgreSQL (free tier : 500MB)
- Variables d'environnement via `.env.local` (JAMAIS commit le .env)

## ğŸ’¡ NOTES IMPORTANTES
- Le projet remplace un workflow Excel manuel
- Objectif : centraliser prospects + Ã©viter doublons
- Amaury gÃ¨re le commercial, partenaire gÃ¨re le technique
- PrivilÃ©gier la simplicitÃ© et la clartÃ© du code
- Interface doit Ãªtre rapide et intuitive pour usage quotidien