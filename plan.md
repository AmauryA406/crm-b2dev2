# Plan de D√©veloppement CRM B2Dev

## Phase 1 : Setup Initial üöß
- [x] Installation Next.js + TypeScript + Tailwind
- [x] Cr√©ation CLAUDE.md (mise √† jour avec specs compl√®tes)
- [x] Cr√©ation plan.md
- [x] Installation Prisma + Supabase
- [x] Cr√©ation sch√©ma BDD (table leads)
- [x] Installation Playwright + xlsx

## Phase 2 : Authentification Next-Auth üîê
- [x] Installation Next-Auth
- [x] Configuration Google OAuth Provider
- [x] Cr√©ation fichier `lib/auth-config.ts` avec AUTHORIZED_EMAILS
- [x] Page `/login` avec bouton "Se connecter avec Google"
- [x] Middleware pour prot√©ger toutes les routes sauf `/login` et `/api/auth`
- [x] Gestion des acc√®s refus√©s (email non autoris√©)
- [x] Tests avec email autoris√© vs non autoris√©
- [x] Layout avec bouton "Se d√©connecter"

## Phase 3 : Validation de Sites Web
- [x] Fonction de d√©tection type de site (annuaire, r√©seaux sociaux, plateforme)
- [x] Fonction de v√©rification responsive (mobile-friendly)
- [x] Fonction de d√©tection date site (avant/apr√®s 2018)
- [x] Tests unitaires de validation

## Phase 4 : Scraping Google Maps
- [x] Setup Playwright
- [x] Fonction de connexion √† Google Maps
- [x] Extraction des donn√©es de base (nom, tel, adresse, ville)
- [x] Extraction note Google + nombre d'avis
- [x] Int√©gration validation site web
- [x] Limite 100 prospects/ville
- [x] V√©rification doublons en temps r√©el
- [x] Gestion des erreurs et timeout

## Phase 5 : API Routes
- [x] POST /api/scrape (lancer le scraping avec validation)
- [x] GET /api/leads (liste + filtres ville/statut/motif)
- [x] POST /api/leads (cr√©er lead manuel)
- [x] GET /api/leads/[id] (d√©tail lead)
- [x] PUT /api/leads/[id] (modifier lead, notamment statut)
- [x] DELETE /api/leads/[id] (supprimer lead)
- [x] POST /api/leads/import (import Excel avec d√©tection doublons)
- [x] GET /api/export (export Excel)

## Phase 6 : Page Scraping
- [x] Layout avec navigation (Scraping / Leads)
- [x] Formulaire : type d'entreprise + villes
- [x] Bouton "Lancer le scraping"
- [x] Affichage temps r√©el des r√©sultats
- [x] afficher une barre de chargement
- [x] Compteur par ville
- [x] Gestion des erreurs utilisateur

## Phase 7 : Page Leads
- [x] Tableau avec colonnes : Nom, Tel, Site, Adresse, Ville, Motif, Statut, Note
- [x] Select inline pour statut (9 options avec emojis)
- [x] Filtres : ville, statut, motif
- [x] Tri par colonne
- [x] Clic sur ligne ‚Üí d√©tail lead
- [x] Bouton "Importer Excel" avec upload
- [x] Bouton "Exporter Excel"
- [x] Affichage des doublons lors de l'import
- [x] Design responsive et professionnel

## Phase 8 : Import/Export Excel
- [x] Parser fichier Excel upload√©
- [x] Mapper colonnes vers sch√©ma BDD
- [x] D√©tection et gestion des doublons
- [x] Rapport d'import (X import√©s, Y doublons)
- [x] Export vers Excel avec toutes les colonnes
- [x] Format r√©utilisable (import/export compatible)

## Phase 9 : Tests & Optimisations
- [ ] Tests du scraping sur diff√©rentes villes
- [ ] Tests import Excel avec doublons
- [ ] Optimisation performances (pagination?)
- [ ] Tests de validation de sites
- [ ] Gestion des cas limites

## Phase 10 : Polish & D√©ploiement
- [ ] Design final Tailwind
- [ ] Messages de succ√®s/erreur clairs
- [ ] Loading states
- [ ] Documentation utilisateur
- [ ] D√©ploiement Vercel + Supabase

---

## D√©cisions Techniques

### 2024-12-13 : Specs Compl√®tes du Projet
- **2 pages principales** : Scraping + Leads
- **Scraping intelligent** : ne garde que sites obsol√®tes/inexistants
- **Gestion doublons** : pendant scraping ET import Excel
- **9 statuts de lead** : du contact initial au projet fini
- **Import/Export Excel** : pour migrer donn√©es existantes

### 2024-12-13 : Choix du Stack
- **Next.js 14** : Framework full-stack (frontend + API)
- **PostgreSQL** : Contraintes UNIQUE pour √©viter doublons
- **Prisma** : ORM moderne avec migrations auto
- **Playwright** : Scraping robuste avec vrai navigateur
- **Supabase** : Hosting PostgreSQL gratuit + interface admin
- **xlsx** : Biblioth√®que pour parsing Excel